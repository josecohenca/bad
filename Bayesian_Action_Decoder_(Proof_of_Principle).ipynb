{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian Action Decoder (Proof-of-Principle).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nU8_qDScYjRY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a proof-of-principle implementation of the Bayesian Action Decoder (BAD) for a two-step matrix game. The entire code can be executed online without downloading."
      ]
    },
    {
      "metadata": {
        "id": "yR0YArThYoVc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Imports.  {vertical-output: true}\n",
        "\n",
        "!pip install dm-sonnet\n",
        "!pip install tfp-nightly\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sonnet as snt\n",
        "import tensorflow as tf \n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N1tfTUf7fkYh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title BAD.  {vertical-output: true}\n",
        "\n",
        "def repeat_tensor(tensor, repetion):\n",
        "  exp_tensor = tf.expand_dims(tensor, -1)\n",
        "  tensor_t = tf.tile(exp_tensor, [1] + repetion)\n",
        "  tensor_r = tf.reshape(tensor_t, repetion * tf.shape(tensor))\n",
        "  return tensor_r\n",
        "\n",
        "\n",
        "def get_ops(bad_mode, payoff_values, batch_size, num_hidden=32):\n",
        "  # Input is a single number for agent 0 and agent 1.\n",
        "  input_0 = tf.placeholder(tf.int32, shape=batch_size)\n",
        "  input_1 = tf.placeholder(tf.int32, shape=batch_size)\n",
        "\n",
        "  # Payoff matrix.\n",
        "  num_cards = payoff_values.shape[0]     # C.\n",
        "  num_actions = payoff_values.shape[-1]  # A.\n",
        "  payoff_tensor = tf.constant(payoff_values)\n",
        "\n",
        "  # Agent 0.\n",
        "  with tf.variable_scope('agent_0'):\n",
        "    weights_0 = tf.get_variable('weights_0', shape=(num_cards, num_actions))\n",
        "    baseline_0_mlp = snt.nets.MLP([num_hidden, 1]) \n",
        "\n",
        "  # Agent 1.\n",
        "  with tf.variable_scope('agent_1'):\n",
        "    p1 = snt.nets.MLP([num_hidden, num_actions])\n",
        "    baseline_1_mlp = snt.nets.MLP([num_hidden, 1])\n",
        "\n",
        "  # These are the 'counterfactual inputs', i.e., all possible cards.\n",
        "  all_inputs = tf.placeholder(tf.int32, shape=(1, num_cards))\n",
        "\n",
        "  # We repeat them for each batch.\n",
        "  repeated_in = tf.reshape(repeat_tensor(all_inputs, [batch_size, 1]), [-1])\n",
        "\n",
        "  # Next we calculate the counterfactual action and log_p for each batch \n",
        "  # and hand.\n",
        "  log_p0 = tf.nn.log_softmax(\n",
        "      tf.matmul(tf.one_hot(repeated_in, num_cards), weights_0))\n",
        "  cf_action = tf.to_int32(\n",
        "      tf.squeeze(tf.multinomial(log_p0, num_samples=1)))  # [BC].\n",
        "  \n",
        "  # Produce log-prob of action selected.\n",
        "  log_cf = tf.reduce_sum(\n",
        "      log_p0 * tf.one_hot(cf_action, num_actions), axis=-1)  # [BC].\n",
        "\n",
        "  # Some reshaping.\n",
        "  repeated_in = tf.reshape(repeated_in, [batch_size, -1])  # [B,C].\n",
        "  cf_action = tf.reshape(cf_action, [batch_size, -1])  # [B,C].\n",
        "  log_cf = tf.reshape(log_cf, [batch_size, -1])  # [B,C].\n",
        "\n",
        "  # Now we need to know the action the agent actually took. \n",
        "  # This is done by indexing the cf_action with the private observation.\n",
        "  u0 = tf.reduce_sum(\n",
        "      cf_action * tf.one_hot(input_0, num_cards, dtype=tf.int32), axis=-1)\n",
        "\n",
        "  # Do the same for the log-prob.\n",
        "  log_p0 = tf.reduce_sum(log_cf * tf.one_hot(input_0, num_cards), axis=-1)\n",
        "\n",
        "  # Joint-action includes all the counterfactual probs - it's simply the sum.\n",
        "  joint_log_p0 = tf.reduce_sum(log_cf, -1)\n",
        "\n",
        "  # Repeating the action chosen so that we can check all matches.\n",
        "  repeated_actions = repeat_tensor(\n",
        "      tf.reshape(u0, [batch_size, 1]), [1, num_cards])\n",
        "\n",
        "  # A hand is possible iff the action in that hand matches the action chosen.\n",
        "  weights = tf.to_int32(tf.equal(cf_action, repeated_actions))\n",
        "\n",
        "  # Normalize beliefs to sum to 1.\n",
        "  beliefs = tf.to_float(\n",
        "      tf.divide(weights, tf.reduce_sum(weights, -1, keepdims=True))) \n",
        "\n",
        "  # Stop gradient mostly as a precaution.\n",
        "  beliefs = tf.stop_gradient(beliefs)\n",
        "\n",
        "  # Agent 1 receives beliefs + private ops for agent 1, unless it's \n",
        "  # the pure policy gradient version.\n",
        "  if bad_mode == 2:\n",
        "    joint_in1 = tf.concat([\n",
        "        tf.one_hot(u0, num_actions, dtype=tf.float32), \n",
        "        tf.one_hot(input_1, num_cards, dtype=tf.float32),\n",
        "    ], axis=1)\n",
        "  else:\n",
        "    joint_in1 = tf.concat([\n",
        "        tf.one_hot(u0, num_actions, dtype=tf.float32),\n",
        "        beliefs,\n",
        "        tf.one_hot(input_1, num_cards, dtype=tf.float32),\n",
        "    ], axis=1)\n",
        "  joint_in1 = tf.reshape(joint_in1, [batch_size, -1])\n",
        "\n",
        "  # We use a centralised baseline that contains both cards as input.\n",
        "  baseline_0_input = tf.concat(\n",
        "      [tf.one_hot(input_0, num_cards), tf.one_hot(input_1, num_cards)], axis=1) \n",
        "  baseline_1_input = tf.concat(\n",
        "      [tf.one_hot(input_0, num_cards), joint_in1], axis=1)\n",
        "\n",
        "  # Calculate baselines.\n",
        "  baseline_0 = baseline_0_mlp(baseline_0_input)\n",
        "  baseline_1 = baseline_1_mlp(baseline_1_input)\n",
        "\n",
        "  # Giving the beliefs a fixed shape so that sonnet doesn't complain \n",
        "  # (probably there's a better way).\n",
        "  beliefs = tf.reshape(beliefs, [batch_size, num_cards])\n",
        "\n",
        "  # Evaluate policy for agent 1.\n",
        "  log_p1 = tf.cast(tf.nn.log_softmax(p1(joint_in1)), tf.float32)\n",
        "\n",
        "  # Sample agent 1 and get log-prob of action selected.\n",
        "  u1 = tf.to_int32(tf.squeeze(tf.multinomial(log_p1, num_samples=1)))\n",
        "  log_p1 = tf.reduce_sum(log_p1 * tf.one_hot(u1, num_actions), axis=-1)\n",
        "\n",
        "  # Getting the rewards is just indexing into the payout matrix for all \n",
        "  # elements in the batch.\n",
        "  rewards = tf.stack([\n",
        "      payoff_tensor[input_0[i], input_1[i], u0[i], u1[i]] \n",
        "      for i in range(batch_size)\n",
        "  ], axis=0)\n",
        "\n",
        "  # Log-prob used for learning.\n",
        "  if bad_mode == 1:\n",
        "    log_p0_train = joint_log_p0\n",
        "  else:\n",
        "    log_p0_train = log_p0\n",
        "  log_p1_train = log_p1\n",
        "\n",
        "  # Policy-gradient loss.\n",
        "  pg_final = tf.reduce_mean(\n",
        "      (rewards - tf.stop_gradient(baseline_0)) * log_p0_train)\n",
        "  pg_final += tf.reduce_mean(\n",
        "      (rewards - tf.stop_gradient(baseline_1)) * log_p1_train)\n",
        "\n",
        "  # Baseline loss.\n",
        "  total_baseline_loss = tf.reduce_mean(tf.square(rewards - baseline_0)) \n",
        "  total_baseline_loss += tf.reduce_mean(tf.square(rewards - baseline_1)) \n",
        "\n",
        "  # Train policy.\n",
        "  opt_policy = tf.train.AdamOptimizer()\n",
        "  train_policy = opt_policy.minimize(-pg_final)\n",
        "  \n",
        "  # Train baseline.\n",
        "  opt_baseline = tf.train.AdamOptimizer()\n",
        "  train_baseline = opt_baseline.minimize(total_baseline_loss)\n",
        "  \n",
        "  # Pack up the placeholders.\n",
        "  phs = {\n",
        "      'input_0': input_0,\n",
        "      'input_1': input_1,\n",
        "      'all_inputs': all_inputs,\n",
        "  }\n",
        "\n",
        "  # Pack up the train ops.\n",
        "  train_ops = {\n",
        "      'policy': train_policy,\n",
        "      'baseline': train_baseline,     \n",
        "  }\n",
        "  \n",
        "  return rewards, train_ops, phs\n",
        "\n",
        "\n",
        "def train(bad_mode,\n",
        "          batch_size=32,\n",
        "          num_runs=1,\n",
        "          num_episodes=5000,\n",
        "          num_readings=100,\n",
        "          seed=42,\n",
        "          debug=False):\n",
        "  # Payoff values, [C,C,A,A].\n",
        "  payoff_values = np.asarray([\n",
        "      [\n",
        "          [[10, 0, 0], [4, 8, 4], [10, 0, 0]],\n",
        "          [[0, 0, 10], [4, 8, 4], [0, 0, 10]],\n",
        "      ],\n",
        "      [\n",
        "          [[0, 0, 10], [4, 8, 4], [0, 0, 0]],\n",
        "          [[10, 0, 0], [4, 8, 4], [10, 0, 0]],\n",
        "      ],\n",
        "  ], dtype=np.float32)\n",
        "  num_cards = payoff_values.shape[0]  # C.\n",
        "\n",
        "  # All cards.\n",
        "  all_cards = np.zeros((1, num_cards))\n",
        "  for i in range(num_cards):\n",
        "    all_cards[0, i] = i\n",
        "\n",
        "  # Reset TF graph.\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  # Set random number generator seeds for reproducibility.\n",
        "  tf.set_random_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  \n",
        "  # Build graph.\n",
        "  rewards_op, train_ops, phs = get_ops(bad_mode, payoff_values, batch_size)\n",
        "  \n",
        "  # Initializer.\n",
        "  init = tf.global_variables_initializer()\n",
        "  \n",
        "  # Run.\n",
        "  rewards = np.zeros((num_runs, num_readings + 1))\n",
        "  interval = num_episodes // num_readings\n",
        "  with tf.Session() as sess:      \n",
        "    for run_id in range(num_runs):\n",
        "      if run_id % max(num_runs // 10, 1) == 0:\n",
        "        print('Run {}/{} ...'.format(run_id + 1, num_runs))\n",
        "      \n",
        "      sess.run(init)\n",
        "      for episode_id in range(num_episodes + 1):\n",
        "        cards_0 = np.random.choice(num_cards, size=batch_size)\n",
        "        cards_1 = np.random.choice(num_cards, size=batch_size)\n",
        "\n",
        "        fetches = [rewards_op, train_ops['baseline'], train_ops['policy']]\n",
        "        feed_dict = {\n",
        "            phs['input_0']: cards_0,\n",
        "            phs['input_1']: cards_1,\n",
        "            phs['all_inputs']: all_cards,\n",
        "        }\n",
        "        reward = sess.run(fetches, feed_dict)[:-2]  # Ignore train ops.\n",
        "        reward = np.mean(reward)  # Average over batch.\n",
        "\n",
        "        # Maybe save.\n",
        "        if episode_id % interval == 0:\n",
        "          rewards[run_id, episode_id // interval] = reward\n",
        "\n",
        "        # Maybe log.\n",
        "        if debug and episode_id % (num_episodes // 5) == 0:\n",
        "          print(episode_id, 'reward:', reward)\n",
        "\n",
        "  return rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2PItU6kTbeYH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Train.  {vertical-output: true}\n",
        "\n",
        "mode_labels = [\n",
        "    'BAD, no CF gradient',\n",
        "    'BAD, with CF gradient', \n",
        "    'Vanilla PG',\n",
        "]\n",
        "\n",
        "# Set debug = True to get a faster run (roughly 5 mins) and more printouts.\n",
        "debug = False\n",
        "\n",
        "if debug:\n",
        "  num_runs = 3\n",
        "  num_episodes = 5000\n",
        "else:\n",
        "  num_runs = 30\n",
        "  num_episodes = 15000\n",
        "num_readings = 100\n",
        "\n",
        "rewards_by_bad_mode = {}\n",
        "for bad_mode in range(3):\n",
        "  print('Running', mode_labels[bad_mode])\n",
        "  rewards_by_bad_mode[bad_mode] = train(bad_mode,\n",
        "                                        num_runs=num_runs,\n",
        "                                        num_episodes=num_episodes,\n",
        "                                        num_readings=num_readings,\n",
        "                                        debug=debug)\n",
        "  print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1FTFPbahZqVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Plot training curves.  {vertical-output: true}\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "save_every = num_episodes // num_readings\n",
        "steps = np.arange(num_readings + 1) * save_every\n",
        "for bad_mode in range(3):\n",
        "  rewards = rewards_by_bad_mode[bad_mode]\n",
        "  mean = rewards.mean(axis=0)\n",
        "  sem = rewards.std(axis=0) / np.sqrt(num_runs)\n",
        "  plt.plot(steps, mean, label=mode_labels[bad_mode])\n",
        "  plt.fill_between(steps, mean - sem, mean + sem, alpha=0.3)\n",
        "plt.ylim(8, 9.7)\n",
        "plt.legend()\n",
        "\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QPRaSjYlXJYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}